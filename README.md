<h1 align="center" style="font-size: 3em;">üëì SignGlasses üëì</h1>

![SLDWORKS_31UnfFawhB](https://github.com/user-attachments/assets/54323b3b-f7d2-437a-94fc-e55f142b69c9)


## Proyecto de Ingenier√≠a Mecatr√≥nica
## Universidad Nacional de Lomas de Zamora 

  **Autores:** 
  - Ignacio Ezequiel Gauna 
  - Juan Pablo Saracino   

**A√±o:** 2025

---

## √çndice

1. [Introducci√≥n](#1-introducci√≥n)  
2. [Funcionalidad del dispositivo](#2-funcionalidad-del-dispositivo)  
   - [2.1. Estructura General del Proyecto](#21-estructura-general-del-proyecto)  
   - [2.2. Descripci√≥n del Flujo de Funciones](#22-descripci√≥n-del-flujo-de-funciones)  
   - [2.3. Sincronizaci√≥n y Operaci√≥n Simult√°nea](#23-sincronizaci√≥n-y-operaci√≥n-simult√°nea)  
3. [Evoluci√≥n del prototipo](#3-evoluci√≥n-del-prototipo)  
4. [Dise√±o electr√≥nico](#4-dise√±o-electr√≥nico)  
5. [Software](#5-software)  
6. [Modelo de Machine Learning y Red Neuronal](#6-modelo-de-machine-learning-y-red-neuronal)  
7. [Ensayos y testeos realizados](#7-ensayos-y-testeos-realizados)  
8. [Costos](#8-costos)  
9. [Mejoras a implementar](#9-mejoras-a-implementar)  
10. [Conclusiones](#10-conclusiones)

---

## 1. Introducci√≥n

El presente informe t√©cnico expone el desarrollo e implementaci√≥n de un sistema port√°til de asistencia comunicacional denominado SignGlasses, un prototipo de lentes inteligentes orientados a la traducci√≥n de lenguaje de se√±as y transcripci√≥n de voz en tiempo real. La iniciativa surge como una soluci√≥n tecnol√≥gica para mejorar la accesibilidad e inclusi√≥n de personas sordas o con dificultades auditivas, facilitando la comunicaci√≥n bidireccional con interlocutores oyentes en entornos cotidianos.  



<p align="center">
  <img width="1015" height="717" alt="photoview360_E2HUJiRA9J" src="https://github.com/user-attachments/assets/da05bcfc-b92f-49c5-a400-691ac0f0aeb5" />
  <br>
  <em>Figura 1: Vista Isom√©trica SignGlasses Model 3 con las patillas cerradas</em>
</p>

El proyecto combina tecnolog√≠as de visi√≥n por computadora y procesamiento de lenguaje natural. El sistema se basa en la utilizaci√≥n de una placa de procesamiento Raspberry Pi, una c√°mara integrada para la detecci√≥n y reconocimiento de se√±as mediante modelos de inteligencia artificial, y una pantalla HUD (Head-Up Display) para la visualizaci√≥n de textos transcritos. Adem√°s, el sistema incorpora un m√≥dulo de reconocimiento de voz que permite la transcripci√≥n instant√°nea del lenguaje hablado hacia texto visible para el usuario.
Este documento detalla el dise√±o, los componentes electr√≥nicos, as√≠ como la arquitectura de software desarrollada para lograr un funcionamiento aut√≥nomo y en tiempo real. El objetivo principal es validar la factibilidad t√©cnica de un dispositivo compacto, accesible y de bajo costo, capaz de asistir en la interpretaci√≥n de lenguaje de se√±as argentino (LSA) y mejorar la experiencia comunicativa de sus usuarios.

---
## 2. Funcionalidad del Dispositivo
### 2.1 Estructura General del Proyecto
El prototipo desarrollado bajo el sistema SignGlasses presenta una doble funcionalidad principal:

- Traducci√≥n en tiempo real de LSA a Espa√±ol oral, permitiendo que la persona sorda adopte el rol de locutor en la comunicaci√≥n.

- Transcripci√≥n en tiempo real de Espa√±ol oral a subt√≠tulos visibles, facilitando que la persona sorda act√∫e como receptor durante una conversaci√≥n con oyentes.


<p align="center">
  <img width="537" height="343" alt="diagrama" src="https://github.com/user-attachments/assets/51741c08-e8b3-4e8e-b444-817e47f33175" />
  <br>
  <em>Figura 2: Diagrama de bloques a nivel macro del funcionamiento de las "SignGlasses"</em>
</p>

### 2.2. Descripci√≥n del Flujo de Funciones
**a) Traducci√≥n de LSA a Espa√±ol (Emisi√≥n):**

Para la funci√≥n de traducci√≥n LSA ‚Üí Espa√±ol, el sistema opera mediante el siguiente flujo t√©cnico:
1.	La captura de se√±as se realiza mediante una c√°mara ubicada sobre un soporte telescopico en los lentes, acompa√±ada de un sistema de seguimiento de la mano en tiempo real, gestionado por un servomotor encargado de orientar el campo visual.
2.	La imagen capturada es procesada inicialmente por la Raspberry Pi, la cual fragmenta el flujo de v√≠deo y lo transmite a una unidad de procesamiento (PC) mediante una comunicaci√≥n basada en UDP sobre USB.
3.	En el PC, se ejecuta un modelo de inteligencia artificial previamente entrenado, utilizando MediaPipe para la detecci√≥n de landmarks de la mano y clasificadores de aprendizaje profundo para la identificaci√≥n de se√±as.
4.	La etiqueta (label) resultante es asociada a un archivo de audio pregrabado correspondiente, el cual es retornado a la Raspberry Pi mediante el mismo canal de comunicaci√≥n.
5.	Finalmente, el audio es reproducido por el sistema de parlantes integrado en los lentes, permitiendo al usuario sordo expresarse de forma oral ante oyentes.

**b) Transcripci√≥n de Espa√±ol Oral a Subt√≠tulos (Recepci√≥n):**

De manera simult√°nea, para la funci√≥n de transcripci√≥n Espa√±ol oral ‚Üí subt√≠tulos, el flujo es el siguiente:
1.	El micr√≥fono INMP441 integrado en los lentes capta fragmentos de audio en intervalos peri√≥dicos de 5 segundos.
2.	La Raspberry Pi procesa el audio entrante y lo transmite a la PC mediante el mismo sistema de transporte UDP.
3.	El audio es procesado por un modelo de reconocimiento de voz (speech-to-text), que convierte las se√±ales sonoras en texto escrito.
4.	El texto transcripto es reenviado a la Raspberry Pi, que lo direcciona al display OLED integrado en la montura, permitiendo al usuario visualizar subt√≠tulos de la conversaci√≥n en tiempo real.

### 2.3. Sincronizaci√≥n y Operaci√≥n Simult√°nea
Ambos procesos (traducci√≥n de se√±as y transcripci√≥n de voz) se ejecutan de forma paralela, gestionados por la Raspberry Pi como n√∫cleo coordinador, brindando al usuario sordo la capacidad de participar activamente en cualquier interacci√≥n comunicativa, tanto emitiendo mensajes orales como recibiendo informaci√≥n visualmente a trav√©s de subt√≠tulos.

<p align="center">
  <img width="523" height="237" alt="Imagen2" src="https://github.com/user-attachments/assets/64a7f7f2-229d-415f-8a2b-0fb6559f2b44" />
  <br>
  <em>Figura 3: Esquema de operaci√≥n simplificado del hardware</em>
</p>

---

## 3. EVOLUCI√ìN DEL PROTOTIPO
### 3.1. Concepto Inicial
El proyecto SignGlasses tuvo su origen en el marco de la c√°tedra de Vigilancia Tecnol√≥gica e Inteligencia Competitiva, donde se elabor√≥ un primer concepto orientado a la identificaci√≥n de una necesidad concreta en materia de accesibilidad comunicacional. En esta etapa inicial, el enfoque fue exclusivamente esquem√°tico, centrado en la conceptualizaci√≥n de los componentes clave del sistema, su l√≥gica de funcionamiento y las posibles aplicaciones. No se contempl√≥ la implementaci√≥n f√≠sica del prototipo, pero esta fase result√≥ fundamental para definir los lineamientos t√©cnicos y funcionales que guiaron el desarrollo posterior del dispositivo.

<p align="center">
  <img width="588" height="340" alt="Imagen3" src="https://github.com/user-attachments/assets/ad5a35c8-b2b8-45c3-989a-07aa445d1cb7" />
  <br>
  <em>Figura 4: Dise√±o conceptual para SignGlasses</em>
</p>

### 3.2. Model 3
El Model 3 representa la iteraci√≥n m√°s reciente y refinada tras un proceso de mejora continua basado en las versiones preliminares, Model 1 y Model 2. A partir de las pruebas realizadas con los prototipos anteriores, se identificaron y resolvieron diversas limitaciones ergon√≥micas y estructurales. Las principales mejoras implementadas incluyen:
- Optimizaci√≥n de la ergonom√≠a y del confort de uso, con un dise√±o que se adapta de manera m√°s estable al cr√°neo y al tabique nasal.
- Incorporaci√≥n de ranuras espec√≠ficas para una mejor gesti√≥n del cableado interno y de las conexiones el√©ctricas.
- Inclusi√≥n de fichas de alimentaci√≥n accesibles y estrat√©gicamente ubicadas para mejorar la portabilidad y la autonom√≠a del sistema.

<p align="center">
  <img width="1015" height="717" alt="photoview360_IQCb9ZApi0" src="https://github.com/user-attachments/assets/c9fbbb66-1602-4398-bb0d-5ff842e49552" />
  <br>
  <em>Figura 5: Vista Isom√©trica de las SignGlasses Model 3</em>
</p>

---

## 4. DISE√ëO ELECTR√ìNICO
### 4.1. Esquema de Conexi√≥n (Fritzing)

<p align="center">
  <img width="1342" height="578" alt="Imagen5" src="https://github.com/user-attachments/assets/916f8cef-a49a-456f-a9c1-284924cc80db" />
  <br>
  <em>Figura 6: Conexionado de los pines de la Raspberry Pi Zero 2w a los diferentes componentes del proyecto</em>
</p>

<p align="center">
  <img width="994" height="703" alt="WINWORD_vi7YMSpowL" src="https://github.com/user-attachments/assets/f398faed-e977-408f-91dd-c3399bc5a005" />
  <br>
  <em>Figura 7: Vista Isom√©trica explosionada con numeraci√≥n de los componentes.</em>
</p>

### 4.2. C√°mara OMNIVISION OV5647
Para la captura de video de los gestos en LSA, se utiliz√≥ el sensor OMNIVISION OV5647 cuenta con CMOS de 5 megap√≠xeles, ampliamente compatible con plataformas de procesamiento como Raspberry Pi a trav√©s de interfaz CSI. La c√°mara cuenta con un lente tipo ojo de pez con una apertura de 130¬∞, lo que permite un √°ngulo de visi√≥n ultra amplio, asegura la captaci√≥n de gestos y movimientos corporales sin necesidad de reposicionar la c√°mara. 

<p align="center">
  <img width="532" height="532" alt="Imagen6" src="https://github.com/user-attachments/assets/34e71159-5077-464a-a1fb-661ce19e5f4a" />
  <br>
  <em>Figura 8: OV5647 imagen de catalogo</em>
</p>

### 4.3. Preamplificador PAM8403
Para la reproducci√≥n de audio traducido (LSA - espa√±ol) se emple√≥ el m√≥dulo amplificador PAM8403, un amplificador de potencia clase D con una capacidad de salida de hasta 3W por canal en est√©reo a 5V. El m√≥dulo integra un potenci√≥metro para el ajuste de volumen, permitiendo al usuario regular la intensidad del audio de salida seg√∫n la intensidad a la cual quiere ser escuchado. Su elecci√≥n se bas√≥ en su alta eficiencia energ√©tica, bajo nivel de distorsi√≥n arm√≥nica (THD), tama√±o compacto y facilidad de integraci√≥n directa con la Raspberry Pi.

<p align="center">
  <img width="1102" height="813" alt="Imagen7" src="https://github.com/user-attachments/assets/15b27c36-7dfa-4ade-80b5-71b58fbc21cb" />
  <br>
  <em>Figura 9: Descripci√≥n de los pines y components del PAM8403</em>
</p>

<p align="center">
  <img width="555" height="342" alt="Imagen8" src="https://github.com/user-attachments/assets/525c0ffb-1638-4f4b-813f-db2b2a39a5e1" />
  <br>
  <em>Figura 10: Circuito esquem√°tico electr√≥nico para el preamplificador PAM8403</em>
</p>

### 4.4. Pantalla OLED 0.96 SSD1306
La visualizaci√≥n del texto transcripto (Speech to Text) se realiza mediante la proyecci√≥n de la pantalla OLED SSD1306 sobre un pl√°stico reflectivo, cuya orientaci√≥n es directa al ojo del usuario. Su tama√±o es de 0.96 pulgadas, con una resoluci√≥n de 128x64 p√≠xeles y comunicaci√≥n mediante protocolo I2C. La tecnolog√≠a OLED permite un alto contraste con bajo consumo energ√©tico, ideal para visores compactos. Su peque√±o tama√±o contribuye a un dise√±o ergon√≥mico y liviano, manteniendo una buena legibilidad incluso en entornos con alta luminosidad gracias al fondo negro caracter√≠stico de este tipo de pantallas. La facilidad de implementaci√≥n mediante librer√≠as ampliamente soportadas en Python fue otro factor determinante para su elecci√≥n. Para versiones futuras se eval√∫a la incorporaci√≥n de sistemas √≥pticos de proyecci√≥n m√°s avanzados, como microproyectores o pantallas transparentes.

<p align="center">
  <img width="541" height="541" alt="Imagen9" src="https://github.com/user-attachments/assets/6e76ab5b-122e-4d24-939c-758b9f8c08f2" />
  <br>
  <em>Figura 11: SSD1306 imagen de cat√°logo</em>
</p>

### 4.5. MICR√ìFONO INMP441
Para la captura de audio se incorpor√≥ el micr√≥fono digital INMP441, un sensor de tipo MEMS (Micro-Electro-Mechanical Systems) con salida digital mediante protocolo I2S, compatible directamente con la Raspberry Pi sin necesidad de conversores anal√≥gicos externos. Este micr√≥fono es omnidireccional, permitiendo captar sonidos provenientes de cualquier direcci√≥n. Su bajo consumo, peque√±o tama√±o y alta sensibilidad lo convierten en una opci√≥n adecuada para aplicaciones port√°tiles como este prototipo. Las principales ventajas son su peque√±o tama√±o y costo.

<p align="center">
  <img width="350" height="193" alt="Imagen10" src="https://github.com/user-attachments/assets/1735a123-5065-4266-bbc1-030956870bf4" />
  <br>
  <em>Figura 12: INMP441 imagen de cat√°logo</em>
</p>

### 4.6. Raspberry Pi Zero 2WH
Como unidad principal de procesamiento se seleccion√≥ la Raspberry Pi Zero 2 WH, una placa compacta que incorpora un procesador quad-core ARM Cortex-A53 a 1 GHz, 512 MB de RAM y conectividad Wi-Fi integrada. Con un precio competitivo se destaca por su excelente relaci√≥n entre capacidad de procesamiento y consumo energ√©tico, soportando sistemas operativos completos basados en Linux. Su principal uso es la comunicaci√≥n de los perif√©ricos con la pc mediante la conexi√≥n USB y el transporte veloz de los datos para una respuesta en tiempo real. Sus interfaces nativas CSI (para c√°mara), I2C, SPI y GPIO facilitan la integraci√≥n directa de los diferentes perif√©ricos del sistema.

<p align="center">
  <img width="1284" height="522" alt="WINWORD_jAAKnYvIFy" src="https://github.com/user-attachments/assets/3844ba6e-194a-439f-90fe-b7e5f2c68d91" />
  <br>
  <em>Figura 13: Distribuci√≥n de pines I/0 de la Raspberry Pi Zero 2W</em>
</p>

### 4.7. MOTOR SERVO SG90
Para la movilidad de la c√°mara se utiliz√≥ el servomotor SG90, un microservo compacto con un √°ngulo de rotaci√≥n de aproximadamente 180¬∞, controlado mediante se√±ales PWM. Su bajo peso, tama√±o reducido y bajo consumo de corriente lo hacen ideal para aplicaciones port√°tiles. En este proyecto, el SG90 es responsable de proporcionar un sistema b√°sico de tracking (seguimiento) permitiendo ajustar la orientaci√≥n de la c√°mara seg√∫n el movimiento del usuario o de la se√±a, aumentando la efectividad del reconocimiento gestual.

<p align="center">
  <img width="1270" height="466" alt="image" src="https://github.com/user-attachments/assets/52203bda-44b8-43dc-9125-5cd3d314676d" />
  <br>
  <em>Figura 14: SG90 Imagen de cat√°logo y composici√≥n interna</em>
</p>

---

## 5. SOFTWARE
El sistema SignGlasses se soporta en un entorno software distribuido que contempla tres m√≥dulos principales, cada uno con una funcionalidad espec√≠fica.
### 5.1. Programa de Recolecci√≥n, Entrenamiento y Administraci√≥n de Se√±as
Este m√≥dulo est√° dise√±ado para la gesti√≥n y entrenamiento del modelo de aprendizaje autom√°tico, ejecutado en una PC. Su objetivo es facilitar la administraci√≥n del dataset y la actualizaci√≥n del modelo neuronal. Las funcionalidades principales son:

**1.	Recolecci√≥n de Nuevas Se√±as**

- Permite registrar nuevas clases de se√±as. El operador asigna un nombre identificativo (label) y el sistema inicia la captura.
- Recibe v√≠deos por UDP desde la Raspberry Pi, procesados en tiempo real mediante MediaPipe, extrayendo 21 landmarks por mano (42 puntos en total).
- Se generan vectores de 126 caracter√≠sticas (3 coordenadas por landmark), muestreados cada 50 ms, alcanzando un total de 5000 frames (aproximadamente 10 minutos de grabaci√≥n).
- El dataset se almacena en formato ‚Äú.pkl‚Äù, incluyendo los vectores y sus respectivas etiquetas, facilitando la carga posterior sin necesidad de regrabar.

**2.	Entrenamiento del Modelo**
- Se encarga de la normalizaci√≥n del dataset mediante StandardScaler, codificaci√≥n de etiquetas con LabelEncoder y entrenamiento del modelo.
- El modelo es un clasificador secuencial denso con capas de 64 y 32 neuronas (activaci√≥n ReLU), Batch Normalization, Dropout y capa de salida Softmax.
- Optimizaci√≥n mediante el algoritmo Adam, guardado en ‚Äú.h5‚Äù y posterior conversi√≥n a formato TensorFlow Lite (.tflite) para reducir la carga computacional en inferencia.
- Genera los gr√°ficos de matriz de confusi√≥n del modelo y las m√©tricas de entrenamiento que permiten validar el modelo entrenado.

<p align="center">
  <img width="1000" height="800" alt="confusion_matrix_20250602_112013" src="https://github.com/user-attachments/assets/39eb752c-5f5f-4b7f-b640-c95104828ba0" />
  <br>
  <em>Figura 17: Matriz de Confusion para el modelo actual de las SignGlasses</em>
</p>

<p align="center">
  <img width="1200" height="500" alt="training_metrics_20250602_112013" src="https://github.com/user-attachments/assets/8ddbea82-61f4-4c51-be48-1e9972065a60" />
  <br>
  <em>Figura 18: Metricas de Entrenamiento para la version actual de las SignGlasses</em>
</p>

**3.	Listado de Se√±as**
- Visualiza el inventario actual de se√±as registradas en el dataset junto a sus etiquetas correspondientes.

**4.	Eliminaci√≥n de Se√±as**
- Permite eliminar registros espec√≠ficos del dataset, ya sea por errores de grabaci√≥n o por bajo rendimiento durante la inferencia.

**5.	Generaci√≥n de Audios**
- Mediante pyttsx3 y pydub se generan archivos de audio .WAV asociados a cada etiqueta registrada, los cuales ser√°n utilizados para la s√≠ntesis de voz en tiempo real.

### 5.2. Programa de Evaluaci√≥n en Tiempo Real
Este programa se ejecuta en la PC y corresponde al entorno final de usuario, enfocado en la inferencia y operaci√≥n en tiempo real, sin permitir la modificaci√≥n del modelo ni del dataset.

**1.	Listado de Se√±as Cargadas**
- Muestra la lista de se√±as habilitadas para inferencia con sus respectivas etiquetas.

**2.	Evaluaci√≥n en Tiempo Real**
- Recibe flujos de im√°genes en fragmentos JPEG por UDP desde la Raspberry Pi, reconstruyendo los frames con OpenCV.
- Procesa los landmarks con MediaPipe, calcula la posici√≥n normalizada de la mu√±eca derecha y env√≠a comandos al servo de seguimiento.
- Normaliza el vector de 126 caracter√≠sticas, ejecuta inferencia con TensorFlow Lite, aplicando StandardScaler y LabelEncoder previamente guardados.
- Si la predicci√≥n supera un umbral de confianza del 90%, muestra en pantalla la etiqueta correspondiente junto con los landmarks y barras de progreso.
- Env√≠a la etiqueta detectada por UDP a la Raspberry Pi para s√≠ntesis de audio, adem√°s de gestionar se√±ales de control como ‚ÄúHANDS_DETECTED‚Äù o ‚ÄúNO_HANDS‚Äù mediante heartbeat para asegurar la robustez del sistema.

### 5.3. Programa Implementado en la Raspberry
El script embebido en la Raspberry Pi cumple la funci√≥n de puente de comunicaci√≥n y procesamiento ligero, realizando tareas de captura y gesti√≥n perif√©rica. Debe trabajar en simultaneo con uno de los dos programas de la PC para cumplir con su funci√≥n:
- Captura v√≠deo desde la c√°mara y lo fragmenta en paquetes UDP de 1460 bytes para transmisi√≥n a la PC.
- Adquiere audio desde el micr√≥fono INMP441 a 48 kHz, realiza remuestreo a 16 kHz y lo transmite cada 5 segundos.
- Reproduce archivos de audio recibidos v√≠a UDP por el altavoz incorporado.
- Recibe √°ngulos de movimiento para el servo SG90 y realiza control en tiempo real del seguimiento.
- Muestra la transcripci√≥n de voz recibida en la pantalla OLED para el usuario sordo.
- Mantiene un sistema de control de estado mediante heartbeat bidireccional, asegurando comunicaci√≥n activa y detecci√≥n de ca√≠das de conexi√≥n.

---

## 6. MODELO DE MACHINE LEARNING Y RED NEURONAL
En este proyecto empleamos aprendizaje autom√°tico para traducir gestos de Lengua de Se√±as Argentina en palabras audibles. El modelo principal es una red neuronal que recibe vectores de caracter√≠sticas extra√≠dos de la mano, mediante el uso del modelo ya creado por Google conocido como mediapipe, el cual discretiza la mano en puntos y l√≠neas, los cuales denominamos ‚Äúlandmarks‚Äù. Nuestro modelo produce una probabilidad para cada clase de se√±a. Esta ‚Äúred‚Äù funciona como un conjunto de capas de procesamiento lineal, donde cada capa transforma su entrada y la pasa a la siguiente, aprendiendo durante el entrenamiento a distinguir patrones en los datos.

### 6.1. Arquitectura implementada:
**Capa de Entrada:** La red se inicia con una capa de entrada que recibe un vector de 126 valores (los 21 landmarks de cada mano, con coordenadas x, y, z). 
**Capas Ocultas:** A continuaci√≥n, una capa densa de 64 neuronas aplica multiplicaci√≥n por pesos, suma de sesgos y la funci√≥n de activaci√≥n ReLU, generando una representaci√≥n intermedia, detecta patrones simples. Una segunda capa densa de 32 neuronas repite este proceso, filtrando la extracci√≥n de caracter√≠sticas o features y detecta patrones complejos. 
**Capa de Salida:** Finalmente, una capa de salida utiliza la activaci√≥n Softmax para convertir las salidas lineales en un vector de probabilidades que suman 1, de donde se selecciona la clase de se√±a con mayor probabilidad. Su funci√≥n es la de clasificar el texto. Cada neurona realiza c√°lculos como: salida = max(0,entrada * peso + sesgos)

<p align="center">
  <img width="586" height="115" alt="Imagen11" src="https://github.com/user-attachments/assets/db279902-6403-4673-9637-caf1c4773d53" />
  <br>
  <em>Figura 19: Arquitectura del modelo</em>
</p>

### 6.2. La red neuronal
En conjunto, el sistema transforma una secuencia de im√°genes en tiempo real en un vector num√©rico, estandariza sus valores, los pasa por una red denominad feed forward entrenada para clasificaci√≥n multiclase, y devuelve el nombre de la se√±a que tenga la mayor confianza o que es lo mismo, el mayor valor de probabilidad. Esa etiqueta se utiliza tanto para generar audio como para integrarse en la interfaz de los lentes, cerrando el ciclo y entregando la traducci√≥n en tiempo real.

<p align="center">
  <img width="416" height="227" alt="Imagen12" src="https://github.com/user-attachments/assets/710bb03d-558d-4dc1-81dc-73eb23c9b928" />
  <br>
  <em>Figura 20: Representacion de una red neuronal tipo feed-forward</em>
</p>

---

## 7. ENSAYOS Y TESTEOS REALIZADOS. 
Con el objetivo de validar el correcto funcionamiento del sistema SignGlasses en condiciones reales de uso, se llevaron a cabo una serie de ensayos experimentales. Los ensayos se realizaron en tres entornos diferenciados: dos de ellos en domicilios particulares, y un tercero en la Facultad de Ingenier√≠a de la Universidad Nacional de Lomas de Zamora, con el fin de obtener resultados representativos bajo distintas condiciones ambientales.

<p align="center">
  <img width="1015" height="717" alt="photoview360_0SocJY1NoR" src="https://github.com/user-attachments/assets/b52b85b2-2c39-4f81-b2c8-ddf4c7bf4194" />
  <br>
  <em>Figura 21: Vista superior y con las patillas cerradas de las SignGlasses Model 3</em>
</p>

### 7.1. Evaluaci√≥n del Impacto de la Iluminaci√≥n:
Las pruebas de funcionamiento evidenciaron que las variaciones en las condiciones de iluminaci√≥n afectan de manera directa la precisi√≥n en la detecci√≥n de se√±as mediante visi√≥n artificial. La intensidad y direcci√≥n de la luz modifican la calidad de la captaci√≥n de im√°genes, lo que repercute en la eficiencia del reconocimiento en tiempo real.

**Acciones implementadas:**
- Se calibr√≥ el umbral de confianza del modelo de reconocimiento gestual basado en MediaPipe, adaptando la sensibilidad del sistema a distintas condiciones lum√≠nicas.
- Se dise√±√≥ e integr√≥ un sistema auxiliar de iluminaci√≥n externa, montado junto a la c√°mara, con el objetivo de estabilizar la captaci√≥n de los puntos clave (landmarks) en ambientes con iluminaci√≥n deficiente o irregular. Esta tan solo fue una prueba, no tiene implementaci√≥n final en las SignGlasses Model 3.

**Observaciones:** La calidad del reconocimiento mejor√≥ significativamente en entornos con iluminaci√≥n controlada. Se recomienda considerar tanto la variabilidad de usuarios como los diferentes entornos lum√≠nicos durante futuras calibraciones.

### 7.2. Calidad de Reproducci√≥n de Audio
En cuanto a la reproducci√≥n de audio a trav√©s del parlante integrado, se observaron mejoras notorias tras la optimizaci√≥n digital de la ganancia. Mediante ajustes por software se logr√≥ incrementar el nivel de salida sin alcanzar la saturaci√≥n del amplificador.

**Optimizaci√≥n aplicada:**
- Incremento de la ganancia digital en +24‚ÄØdB durante la etapa de post-procesamiento del audio.

**Resultados:**
- Se obtuvo un aumento del volumen percibido, mejorando la relaci√≥n se√±al/ruido (SNR) sin generar distorsi√≥n o saturaci√≥n prematura.
- La inteligibilidad del audio fue mejorada, generando una experiencia de usuario m√°s clara y efectiva.

**Conclusi√≥n:** La implementaci√≥n de ganancia digital controlada permiti√≥ optimizar la calidad sonora sin comprometer la integridad del sistema de amplificaci√≥n.

### 7.3. Visibilidad de la Pantalla
Las pruebas sobre la pantalla OLED reflejaron limitaciones en situaciones de alta luminosidad ambiente. La baja capacidad de brillo gener√≥ problemas de legibilidad especialmente a contraluz, afectando la percepci√≥n del contenido proyectado.

**Optimizaci√≥n implementada:** Incorporaci√≥n de un filtro polarizado sobre la superficie reflectante, lo cual permiti√≥ atenuar la incidencia de luz externa y mejorar la legibilidad del texto proyectado.

**Observaciones:** Si bien se mejor√≥ parcialmente la visibilidad mediante soluciones √≥pticas pasivas, se recomienda para futuras iteraciones explorar pantallas transparentes o sistemas √≥pticos m√°s avanzados de proyecci√≥n.

### 7.4. Validaci√≥n de Modelos de Machine Learning
Se realizaron ensayos comparativos con diferentes configuraciones de modelos de aprendizaje profundo para el reconocimiento gestual.

**Pruebas realizadas:**
- Modelos basados en redes neuronales tipo MLP, CNN, RNN LSTM y Transformer.
- Evaluaci√≥n de distintas arquitecturas, variando el n√∫mero de capas ocultas, cantidad de neuronas, tipo de caracter√≠sticas de entrada e hiperparametros.
- Comparaci√≥n del rendimiento en funci√≥n del tiempo de respuesta y precisi√≥n de reconocimiento.

**Resultado final:** La mejor configuraci√≥n se describe en el √çtem 4 del presente informe, donde se obtuvo un equilibrio √≥ptimo entre precisi√≥n, velocidad de inferencia y consumo de recursos.

---

## 8 COSTOS
### 8.1. Costo real del prototipo

| Componente                         | Precio ARS | Precio USD | Proveedor            | Fecha de Compra | Fecha de Entrega |
|-----------------------------------|------------|------------|----------------------|------------------|------------------|
| C√°mara OV4547 ojo de pez 5MP      | $10.829    | $10,18     | Starware             | 15/01/2025       | 20/01/2025       |
| Cable c√°mara Raspberry Pi 15cm    | $1.959     | $1,84      | Starware             | 15/01/2025       | 20/01/2025       |
| Amplificador audio PAM8403        | $2.239     | $2,10      | Starware             | 15/01/2025       | 20/01/2025       |
| Pantalla OLED SSD1306 0.96"       | $5.549     | $5,21      | Starware             | 15/01/2025       | 20/01/2025       |
| Raspberry Pi Zero 2WH             | $38.957    | $36,61     | Siranet              | 09/01/2025       | 11/01/2025       |
| Micr√≥fono INMP441                 | $7.676     | $7,21      | Synergeek            | 26/03/2025       | 31/03/2025       |
| **Total**                         | **$67.209**| **$63,16** |                      |                  |                  |

> D√≥lar de referencia: 1064,15 ARS/USD

### 8.2. Costo estimado del proyecto real
El costo del prototipo funcional es de aproximadamente $67.209 ARS o 63,16 USD (con un d√≥lar de referencia a 1064,15 ARS/USD).

Este costo contempla solo unidades sueltas de cada componente y sin optimizaci√≥n de producci√≥n.

En un proyecto real, deber√≠as considerar:
- **Producci√≥n a escala** (baja, media o alta).
- **Costos adicionales de desarrollo:** dise√±o de PCB, manufactura de carcasa espec√≠fica, empaquetado de hardware.
- **Costos de software:** licencias, mantenimiento y actualizaciones.
- **Log√≠stica:** embalaje, distribuci√≥n y soporte post-venta.
- **Calidad industrial:** certificaciones, pruebas de fiabilidad.

Una regla habitual para pasar de prototipo a producto es considerar un factor multiplicador que oscila entre 3 y 10 veces el costo de prototipo, dependiendo de la escala y complejidad del producto.

**Estimaci√≥n Final Aproximada:**
- Para una producci√≥n en peque√±a escala o MVP comercializable (por ejemplo, 20 a 100 unidades), se podr√≠a estimar un costo unitario cercano a los 250 - 400 USD.
- En pesos argentinos, con la misma referencia cambiaria, ser√≠a aproximadamente 266.000 - 426.000 ARS por unidad.
- Esto contempla materiales, impresi√≥n 3D o carcasas moldeadas, PCBs integradas, optimizaci√≥n de cableado y encapsulado, y una primera versi√≥n estable de software.
- Si el desarrollo apunta a una producci√≥n a mayor escala (>1000 unidades) y se optimiza dise√±o y producci√≥n, el costo podr√≠a reducirse por econom√≠a de escala, pero se sumar√≠an costos iniciales fijos elevados (moldes, certificaciones).

---

## 9. MEJORAS A IMPLEMENTAR 
A partir de los ensayos realizados y el an√°lisis funcional del prototipo, se identificaron una serie de posibles mejoras t√©cnicas destinadas a optimizar el rendimiento general del sistema SignGlasses. Estas mejoras abarcan aspectos de hardware, software, procesamiento, ergonom√≠a y escalabilidad futura del dispositivo.

<p align="center">
  <img width="1015" height="717" alt="photoview360_l0Pu0s7PTx" src="https://github.com/user-attachments/assets/4259c911-e2c2-4929-99fe-5e475b805e64" />
  <br>
  <em>Figura 22: Vista trasera y explosionada de las SignGlasses Model 3</em>
</p>

### 9.1. Pantalla
**Propuestas t√©cnicas:**
- Incorporaci√≥n de una pantalla de alto brillo con control din√°mico de retroiluminaci√≥n, para garantizar una correcta visualizaci√≥n en ambientes exteriores y bajo condiciones de contraluz.
- Implementaci√≥n de un display transparente OLED integrado en la montura, permitiendo la proyecci√≥n directa sobre el campo visual del usuario sin obstrucci√≥n.
- Evaluaci√≥n de un sistema avanzado de proyecci√≥n √≥ptica, similar a los utilizados en dispositivos de realidad aumentada o realidad virtual.

**Resultado esperado:**
Mejorar significativamente la legibilidad de la informaci√≥n proyectada bajo cualquier condici√≥n lum√≠nica, adem√°s de reducir tama√±o y peso del m√≥dulo √≥ptico, acercando el prototipo a un dise√±o m√°s compacto y discreto.

### 9.2. Micr√≥fono
**Propuestas t√©cnicas:**
- Sustituci√≥n del micr√≥fono omnidireccional por un micr√≥fono direccional, con mayor selectividad para captar √∫nicamente voces humanas.
- Implementaci√≥n de t√©cnicas de preprocesamiento digital, incluyendo cancelaci√≥n de eco y supresi√≥n adaptativa de ruido mediante software.
- Evaluaci√≥n de micr√≥fonos de alta sensibilidad y mayor alcance efectivo, con capacidades de diferenciaci√≥n de fuentes sonoras para futuras funciones avanzadas de identificaci√≥n de locutor.

**Resultado esperado:**
Mayor precisi√≥n en la transcripci√≥n de voz, mejor desempe√±o en entornos ruidosos y ampliaci√≥n del rango operativo efectivo hasta 5 metros, sin degradaci√≥n en la calidad del reconocimiento de audio.

### 9.3. Sistema de Audio
**Propuestas t√©cnicas:**
- Reemplazo del actual m√≥dulo de audio por parlantes integrados de mayor potencia, posicionados lateralmente en el armaz√≥n.
- Incorporaci√≥n de un sistema t√°ctil de control de volumen o interfaz m√°s compacta, eliminando potenci√≥metros f√≠sicos.
- Visualizaci√≥n en pantalla del nivel de volumen para una interacci√≥n m√°s intuitiva.

**Resultado esperado:**
Mejorar la calidad y claridad del audio emitido, reducir distorsi√≥n y proporcionar una experiencia auditiva m√°s personalizable y est√©tica para el usuario.

### 9.4. Procesamiento.
#### 9.4.1. Ejecuci√≥n en dispositivo m√≥vil
**Propuestas t√©cnicas:**
- Portar el modelo de inferencia a TensorFlow Lite para su ejecuci√≥n en Android/iOS.
- Integrar un sistema de comunicaci√≥n ligera mediante WiFi, Bluetooth o MQTT con el hardware principal.

**Resultado esperado:**
Desvincular el procesamiento de computadoras externas, reduciendo la latencia, mejorando la portabilidad y explotando la capacidad de c√≥mputo de dispositivos m√≥viles modernos.

#### 9.4.2. Despliegue en servidor web
**Propuestas t√©cnicas:**
- Servidor en la nube AWS(Amazon) o GCP(Google) que reciba im√°genes y devuelva la etiqueta de se√±a.
- Servidores remotos, conexi√≥n WAN. El servidor que procese el programa y los modelos ML, esta ubicado en un determinado lugar.

**Resultado esperado:**
Reducir la carga de procesamiento local, permitiendo mayor escalabilidad y mantenimiento centralizado del modelo de IA.

#### 9.4.3. Sistema Embebido
**Propuestas t√©cnicas:**
- Desarrollo de un sistema totalmente embebido utilizando placas avanzadas con procesadores integrados de mayor rendimiento.

**Resultado esperado:
Eliminar completamente la dependencia de dispositivos externos, obteniendo un prototipo completamente aut√≥nomo.

### 9.5. Enriquecimiento de la traducci√≥n
#### 9.5.1. Precisi√≥n y Latencia
**Propuestas t√©cnicas:**
- Mejorar el modelo de la red neuronal, en base a un servidor con mejor poder de procesamiento.
- Aumentar la cantidad de frames por secuencias, como la cantidad de secuencias por se√±a. 
- Obtener diversas grabaciones y recolecciones de diversas fuentes. Como distintas personas, distintas locaciones. 

**Resultado esperado:**
Mejorar la robustez y exactitud de la traducci√≥n, garantizando detecci√≥n efectiva sin importar la persona o locaci√≥n.

#### 9.5.2. Contextualizaci√≥n Ling√º√≠stica
La traducci√≥n palabra a palabra en LSA carece de conectores y fluidez gramatical.

**Propuesta t√©cnica:**
- Mediante el modelo de lenguaje basado en RNN/LSTM, se puede ir tomando las se√±as, llenar un array, y que la IA complete mediante los conectores y despu√©s que lo reproduzca todo junto. Si la capacidad de procesamiento es la adecuada no se ver√° reflejado en retrasos entre hacer las se√±as y emitir el texto. Seria en tiempo real.
- Post procesamiento en tiempo real: rellenar conectores (‚Äúme gustar√≠a que, el auto es rojo, y ajustar el orden sint√°ctico seg√∫n reglas del espa√±ol.

**Resultado esperado:**
- Generaci√≥n de oraciones naturales, mejor comprensi√≥n y mayor aceptaci√≥n social del sistema.
- Ejemplo antes/despu√©s:

Sin IA contextual: ‚ÄúHola gustar color cielo‚Äù

Con IA contextual: ‚ÄúHola, me gusta el color del cielo.‚Äù

### 9.6. Reducci√≥n de Tama√±o y Peso
**Propuesta t√©cnica:**
- Optimizaci√≥n del dise√±o mec√°nico utilizando materiales m√°s livianos y resistentes, como pol√≠meros t√©cnicos o aleaciones livianas.
- Reorganizaci√≥n interna de componentes para una mejor distribuci√≥n por medio de un PCB con tama√±o y forma de lentes completamente funcional.

**Resultado esperado:**
Conseguir un dise√±o ergon√≥mico y liviano, visualmente similar a anteojos convencionales.

### 9.7. Sistema Inal√°mbrico
**Propuestas t√©cnicas:**
- Incorporaci√≥n de bater√≠as integradas recargables y reemplazo total de conexiones f√≠sicas mediante enlaces inal√°mbricos.
**Resultado esperado:**
Dispositivo completamente inal√°mbrico, con mayor autonom√≠a y libertad de movimiento para el usuario.

### 9.8. Incorporar reconocimiento de expresiones faciales.
**Propuestas t√©cnicas:**
- Implementar el m√≥dulo MediaPipe Face, para el reconocimiento de expresiones faciales y gestos asociados al rostro.
- Ampliar la l√≥gica de procesamiento para integrar la informaci√≥n facial con la manual.

**Resultado esperado:**
Expandir el diccionario de se√±as incluyendo expresiones faciales, fundamentales en la sem√°ntica del LSA.

### 9.9. Diccionario LSA Completo
**Propuestas t√©cnicas:**
- Implementar el m√≥dulo MediaPipe Face, para el reconocimiento de expresiones faciales y gestos asociados al rostro.
- Ampliar la l√≥gica de procesamiento para integrar la informaci√≥n facial con la manual.

**Resultado esperado:**
Expandir el diccionario de se√±as incluyendo expresiones faciales, fundamentales en la sem√°ntica del LSA.

### 9.10. Visi√≥n evolutiva:
La implementaci√≥n de estas mejoras permitir√° establecer una hoja de ruta de evoluci√≥n tecnol√≥gica, orientada a transformar el actual prototipo en un producto final robusto, aut√≥nomo, est√©tico y funcional, capaz de ofrecer asistencia comunicacional en diversos entornos de uso, promoviendo la inclusi√≥n y la accesibilidad de las personas sordas o hipoac√∫sicas en la vida cotidiana.

---

## 10. CONCLUSIONES 
Analizando el grado de avance alcanzado en el desarrollo del prototipo, se concluye que el sistema SignGlasses ha logrado satisfacer los objetivos t√©cnicos planteados en la etapa inicial. El dispositivo demostr√≥ ser capaz de traducir correctamente un conjunto de al menos 20 se√±as distintas del Lenguaje de Se√±as Argentino (LSA), alcanzando un nivel de precisi√≥n del 93‚ÄØ% en condiciones controladas. 

<p align="center">
  <img width="1015" height="718" alt="photoview360_2VCX7lxVye" src="https://github.com/user-attachments/assets/c8af8251-fc45-4af6-8946-9b1cb355ef39" />
  <br>
  <em>Figura 23: Vista Superior de las SignGlasses Model 3</em>
</p>

Sin embargo, a lo largo del presente informe se ha evidenciado que el sistema cuenta con un amplio margen de mejora, especialmente en t√©rminos de robustez, escalabilidad y capacidad de adaptaci√≥n a entornos no controlados. Desde una perspectiva funcional, el prototipo cumple satisfactoriamente con su prop√≥sito fundamental: traducir en tiempo real se√±as a palabras audibles, permitiendo que personas sordas puedan comunicarse de forma m√°s efectiva con interlocutores oyentes. El dise√±o en formato de lentes representa un acierto ergon√≥mico, ya que libera las manos del usuario, respetando la naturalidad del lenguaje de se√±as y facilitando su uso prolongado.

No obstante, se identificaron limitaciones relacionadas con la formaci√≥n gramatical de oraciones, dado que la versi√≥n actual realiza traducci√≥n palabra a palabra sin conectores sint√°cticos. Este aspecto constituye una oportunidad de mejora a nivel de software, lo cual representa una ventaja significativa al evitar la necesidad de redise√±os en el hardware existente, y facilitar actualizaciones a futuro mediante simples revisiones del modelo de inteligencia artificial.
Desde el punto de vista econ√≥mico, el prototipo se ha mantenido dentro de un rango de costos accesible, considerando que se trata de una fabricaci√≥n artesanal y sin optimizaci√≥n por escala de producci√≥n. Es esperable que, mediante fabricaci√≥n en serie o adquisici√≥n de componentes en volumen, se logre una reducci√≥n significativa del costo unitario. Asimismo, la futura migraci√≥n parcial o total del procesamiento hacia plataformas m√≥viles o servicios en la nube abrir√≠a nuevas oportunidades para abaratar el hardware f√≠sico y optimizar la autonom√≠a del dispositivo

Finalmente, desde un enfoque social y de accesibilidad, el proyecto SignGlasses demuestra un impacto potencial considerable dentro de la comunidad sorda e hipoac√∫sica, facilitando la integraci√≥n social sin la necesidad de que el entorno domine el lenguaje de se√±as. Cabe destacar que este desarrollo no busca reemplazar la ense√±anza del lenguaje de se√±as, sino brindar una herramienta complementaria que reduzca las barreras comunicativas en situaciones cotidianas, ofreciendo mayor independencia a los usuarios.
En s√≠ntesis, el prototipo desarrollado representa un primer paso s√≥lido y funcional en la b√∫squeda de una soluci√≥n tecnol√≥gica accesible y eficaz. Con cada iteraci√≥n futura se apunta a cerrar progresivamente la brecha comunicacional, acerc√°ndose a un producto final con mayor precisi√≥n, mejor portabilidad y un dise√±o m√°s amigable para el uso diario.



---
## ü§ù Contacto

- **Ignacio Ezequiel Gauna**  
  - üì¨Email:‚ÄØgauna.ignaciosh@hotmail.com  
  - GitHub:‚ÄØ[@IgnacioGauna](https://github.com/StylHard)

- **Juan Pablo Saracino**  
  - üì¨Email:‚ÄØsaracinojuanpablo@gmail.com  
  - GitHub:‚ÄØ[@SaracinoJuanPablo](https://github.com/JuanPa2023)

**Facultad de Ingenier√≠a ‚Äì UNLZ**  
_Proyecto: SignGlasses_

